{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e29562",
   "metadata": {},
   "source": [
    "# Lecture-1: Overview and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fff1c3",
   "metadata": {},
   "source": [
    "In the first lecture of CS-336 we mainly studied about **tokenization** and with the focus of mainly at **BPE (Byte-Pair Encoding) tokenizer**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85aa483",
   "metadata": {},
   "source": [
    "## 1. Intro To Tokenization\n",
    "\n",
    "When we talk about Language Models we consider them like a giant math functions. They don't understand \"words\"; they understand numbers. We need a way to turn `The quick brown fox` into something like `[42, 512, 999, 204]`. This is where tokenization comes into play. Tokenization is the process of breaking a stream of raw text (like above example) into smaller and discrete units called tokens. <br>\n",
    "A language model places a proabbility distribution over sequence of tokens. Hence, we need a procedure that encodes strings into tokens and also need a procedure that decodes tokens back into strings. A **tokenizer** is a class that implements the encode and decode methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f06225",
   "metadata": {},
   "source": [
    "### 1.1 Approach-1: Character Based Tokenization\n",
    "\n",
    "Before fancy algorithms like BPE, WordPiece and Unigram came there existed the most fundamental tokenization method of all which was tokenizing text at the character level. In the layman terms this concept sounds too simple and that's the point of character based tokenization.<br>\n",
    "\n",
    "In character based tokenization our token is literally a single character.<br>\n",
    "For example the sentence:\n",
    "```css\n",
    "hello world\n",
    "```\n",
    "gets tokenized as:\n",
    "```css\n",
    "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
    "```\n",
    "\n",
    "Every letter or punctuation mark, whitespace and symbol becomes its own token. That means the tokenizer’s vocabulary is simply:\n",
    "- All letters (a–z, A–Z)\n",
    "- All digits (0–9)\n",
    "- All punctuation and symbols (!, @, #, $, …)\n",
    "- All whitespace types\n",
    "- Any special tokens you define (e.g., BOS, EOS)\n",
    "\n",
    "So the vocabulary size might be ~100–300 tokens depending on the language hence the first drawback of this method is that it is is tiny in comparison.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
