{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e29562",
   "metadata": {},
   "source": [
    "# Lecture-1: Overview and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fff1c3",
   "metadata": {},
   "source": [
    "In the first lecture of CS-336 we mainly studied about **tokenization** and with the focus of mainly at **BPE (Byte-Pair Encoding) tokenizer**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85aa483",
   "metadata": {},
   "source": [
    "## 1. Intro To Tokenization\n",
    "\n",
    "When we talk about Language Models we consider them like a giant math functions. They don't understand \"words\"; they understand numbers. We need a way to turn `The quick brown fox` into something like `[42, 512, 999, 204]`. This is where tokenization comes into play. Tokenization is the process of breaking a stream of raw text (like above example) into smaller and discrete units called tokens. <br>\n",
    "A language model places a proabbility distribution over sequence of tokens. Hence, we need a procedure that encodes strings into tokens and also need a procedure that decodes tokens back into strings. A **tokenizer** is a class that implements the encode and decode methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f06225",
   "metadata": {},
   "source": [
    "### 1.1 Approach-1: Character Based Tokenization\n",
    "\n",
    "Before fancy algorithms like BPE, WordPiece and Unigram came there existed the most fundamental tokenization method of all which was tokenizing text at the character level. In the layman terms this concept sounds too simple and that's the point of character based tokenization.<br>\n",
    "\n",
    "In character based tokenization our token is literally a single character.<br>\n",
    "For example the sentence:\n",
    "```css\n",
    "hello world\n",
    "```\n",
    "\n",
    "gets tokenized as:\n",
    "\n",
    "```css\n",
    "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
    "```\n",
    "\n",
    "Every letter or punctuation mark, whitespace and symbol becomes its own token. That means the tokenizer’s vocabulary is simply:\n",
    "- All letters (a–z, A–Z)\n",
    "- All digits (0–9)\n",
    "- All punctuation and symbols (!, @, #, $, …)\n",
    "- All whitespace types\n",
    "- Any special tokens you define (e.g., BOS, EOS)\n",
    "\n",
    "So the vocabulary size might be ~100–300 tokens depending on the language hence the first drawback of this method is that it is is tiny in comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf936244",
   "metadata": {},
   "source": [
    "#### Some strengths of this approach\n",
    "\n",
    "1. **Zero Out-of-Vocabulary Issues**:\n",
    "\n",
    "With word-level tokenizers if we encounter a rare word like:\n",
    "```css\n",
    "supercalifragilisticexpialidocious\n",
    "```\n",
    "then the model can’t represent it as a single token unless it was already in the vocabulary. But character level tokenization solves this probelm. It breaks the above word into:\n",
    "```css\n",
    "['s', 'u', 'p', 'e', 'r', ...]\n",
    "```\n",
    "and makes everything representable.\n",
    "\n",
    "2. **Useful in Low-Data Settings**\n",
    "\n",
    "If we are training a small RNN, a tiny GPT-like model from scratch(nanoGPT experiments), or a language model for a very narrow domain then small vocabulary helps the model converge faster and this is where character based approach is much useful. Karpathy’s early [char-RNNs for generating Shakespeare](https://github.com/karpathy/char-rnn) relied on this simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc406f4",
   "metadata": {},
   "source": [
    "#### But Then What’s the Drawback?\n",
    "\n",
    "1. **Sequence Length Explosion:**\n",
    "\n",
    "Consider the below example:\n",
    "\n",
    "```bash\n",
    "hello world\n",
    "```\n",
    "\n",
    "If we see here clearly there are `11` character level tokens. Now imagine if we are doing some real life language modelling hence then we need real sentences, paragraphs or books and then model will need to process much longer sequences. Longer sequence will mean more memory, more training time, fewer tokens per batch (slower learning) and also more attention computation (quadratic cost). Hence ** this is the single biggest reason modern LLMs don’t use pure character tokenization.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
