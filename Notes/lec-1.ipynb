{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e29562",
   "metadata": {},
   "source": [
    "# Lecture-1: Overview and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fff1c3",
   "metadata": {},
   "source": [
    "In the first lecture of CS-336 we mainly studied about **tokenization** and with the focus of mainly at **BPE (Byte-Pair Encoding) tokenizer**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85aa483",
   "metadata": {},
   "source": [
    "## 1. Intro To Tokenization\n",
    "\n",
    "When we talk about Language Models we consider them like a giant math functions. They don't understand \"words\"; they understand numbers. We need a way to turn `The quick brown fox` into something like `[42, 512, 999, 204]`. This is where tokenization comes into play. Tokenization is the process of breaking a stream of raw text (like above example) into smaller and discrete units called tokens. <br>\n",
    "A language model places a proabbility distribution over sequence of tokens. Hence, we need a procedure that encodes strings into tokens and also need a procedure that decodes tokens back into strings. A **tokenizer** is a class that implements the encode and decode methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f06225",
   "metadata": {},
   "source": [
    "### 1.1 Approach-1: Character Based Tokenization\n",
    "\n",
    "Before fancy algorithms like BPE, WordPiece and Unigram came there existed the most fundamental tokenization method of all which was tokenizing text at the character level. In the layman terms this concept sounds too simple and that's the point of character based tokenization.<br>\n",
    "\n",
    "In character based tokenization our token is literally a single character.<br>\n",
    "For example the sentence:\n",
    "```css\n",
    "hello world\n",
    "```\n",
    "\n",
    "gets tokenized as:\n",
    "\n",
    "```css\n",
    "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
    "```\n",
    "\n",
    "Every letter or punctuation mark, whitespace and symbol becomes its own token. That means the tokenizer’s vocabulary is simply:\n",
    "- All letters (a–z, A–Z)\n",
    "- All digits (0–9)\n",
    "- All punctuation and symbols (!, @, #, $, …)\n",
    "- All whitespace types\n",
    "- Any special tokens you define (e.g., BOS, EOS)\n",
    "\n",
    "So the vocabulary size might be ~100–300 tokens depending on the language hence the first drawback of this method is that it is is tiny in comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf936244",
   "metadata": {},
   "source": [
    "#### Some strengths of this approach\n",
    "\n",
    "1. **Zero Out-of-Vocabulary Issues**:\n",
    "\n",
    "With word-level tokenizers if we encounter a rare word like:\n",
    "```css\n",
    "supercalifragilisticexpialidocious\n",
    "```\n",
    "then the model can’t represent it as a single token unless it was already in the vocabulary. But character level tokenization solves this probelm. It breaks the above word into:\n",
    "```css\n",
    "['s', 'u', 'p', 'e', 'r', ...]\n",
    "```\n",
    "and makes everything representable.\n",
    "\n",
    "2. **Useful in Low-Data Settings**\n",
    "\n",
    "If we are training a small RNN, a tiny GPT-like model from scratch(nanoGPT experiments), or a language model for a very narrow domain then small vocabulary helps the model converge faster and this is where character based approach is much useful. Karpathy’s early [char-RNNs for generating Shakespeare](https://github.com/karpathy/char-rnn) relied on this simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc406f4",
   "metadata": {},
   "source": [
    "#### But Then What’s the Drawback?\n",
    "\n",
    "1. **Sequence Length Explosion:**\n",
    "\n",
    "Consider the below example:\n",
    "\n",
    "```bash\n",
    "hello world\n",
    "```\n",
    "\n",
    "If we see here clearly there are `11` character level tokens. Now imagine if we are doing some real life language modelling hence then we need real sentences, paragraphs or books and then model will need to process much longer sequences. Longer sequence will mean more memory, more training time, fewer tokens per batch (slower learning) and also more attention computation (quadratic cost). Hence **this is the single biggest reason modern LLMs don’t use pure character tokenization.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e6f3a",
   "metadata": {},
   "source": [
    "### 1.2 Approach-2: Word Based Tokenization\n",
    "\n",
    "After failure of character based tokenization failure the researchers thought that **A text is just a sequence of words. So what if we tokenize the text by splitting it into words.** This idea is very natural because that’s how humans think but then machines aren't humans and they don't think in words and that's why this mismatch turned word-based tokenization from NLP’s first “obvious solution” into one of its biggest limitations.<br>\n",
    "\n",
    "#### What is Word-Based Tokenization?\n",
    "\n",
    "In simple explanation word tokenization splits text at whitespace and punctuation into words. For example:<br>\n",
    "\n",
    "```bash\n",
    "Input:   \"Hello, world! I’m learning NLP.\"\n",
    "Tokens:  [\"Hello\", \"world\", \"I’m\", \"learning\", \"NLP\"]\n",
    "```\n",
    "\n",
    "The tokenizer removes punctuation or isolates it and then everything else becomes a word. After this each unique word becomes a vocabulary entry and then each word maps to an integer ID and finally model processes sequences of those integer IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ee9ff",
   "metadata": {},
   "source": [
    "#### How This Approach Fails:\n",
    "\n",
    "1. **Vocabulary Explosion:**\n",
    "\n",
    "When every unique word becomes an entry in the vocabulary then even simple variations produce new tokens. For example words like below:\n",
    "\n",
    "```css\n",
    "apple\n",
    "apples\n",
    "Apple\n",
    "APPLE\n",
    "apple's\n",
    "apple-like\n",
    "```\n",
    "will generate new tokens with even simple variations and this will result in size explosion.\n",
    "\n",
    "\n",
    "2. **Out of Vocabulary (OOV):**\n",
    "\n",
    "If a word never appeared during training the model cannot represent it. Hence the model can’t learn representations for words that don’t exist in training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
