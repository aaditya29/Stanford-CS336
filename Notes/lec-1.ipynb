{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e29562",
   "metadata": {},
   "source": [
    "# Lecture-1: Overview and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fff1c3",
   "metadata": {},
   "source": [
    "In the first lecture of CS-336 we mainly studied about **tokenization** and with the focus of mainly at **BPE (Byte-Pair Encoding) tokenizer**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85aa483",
   "metadata": {},
   "source": [
    "## 1. Intro To Tokenization\n",
    "\n",
    "When we talk about Language Models we consider them like a giant math functions. They don't understand \"words\"; they understand numbers. We need a way to turn `The quick brown fox` into something like `[42, 512, 999, 204]`. This is where tokenization comes into play. Tokenization is the process of breaking a stream of raw text (like above example) into smaller and discrete units called tokens. <br>\n",
    "A language model places a proabbility distribution over sequence of tokens. Hence, we need a procedure that encodes strings into tokens and also need a procedure that decodes tokens back into strings. A **tokenizer** is a class that implements the encode and decode methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f06225",
   "metadata": {},
   "source": [
    "### 1.1 Approach-1: Character Based Tokenization\n",
    "\n",
    "Before fancy algorithms like BPE, WordPiece and Unigram came there existed the most fundamental tokenization method of all which was tokenizing text at the character level. In the layman terms this concept sounds too simple and that's the point of character based tokenization.<br>\n",
    "\n",
    "In character based tokenization our token is literally a single character.<br>\n",
    "For example the sentence:\n",
    "```css\n",
    "hello world\n",
    "```\n",
    "\n",
    "gets tokenized as:\n",
    "\n",
    "```css\n",
    "['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
    "```\n",
    "\n",
    "Every letter or punctuation mark, whitespace and symbol becomes its own token. That means the tokenizer’s vocabulary is simply:\n",
    "- All letters (a–z, A–Z)\n",
    "- All digits (0–9)\n",
    "- All punctuation and symbols (!, @, #, $, …)\n",
    "- All whitespace types\n",
    "- Any special tokens you define (e.g., BOS, EOS)\n",
    "\n",
    "So the vocabulary size might be ~100–300 tokens depending on the language hence the first drawback of this method is that it is is tiny in comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf936244",
   "metadata": {},
   "source": [
    "#### Some strengths of this approach\n",
    "\n",
    "1. **Zero Out-of-Vocabulary Issues**:\n",
    "\n",
    "With word-level tokenizers if we encounter a rare word like:\n",
    "```css\n",
    "supercalifragilisticexpialidocious\n",
    "```\n",
    "then the model can’t represent it as a single token unless it was already in the vocabulary. But character level tokenization solves this probelm. It breaks the above word into:\n",
    "```css\n",
    "['s', 'u', 'p', 'e', 'r', ...]\n",
    "```\n",
    "and makes everything representable.\n",
    "\n",
    "2. **Useful in Low-Data Settings**\n",
    "\n",
    "If we are training a small RNN, a tiny GPT-like model from scratch(nanoGPT experiments), or a language model for a very narrow domain then small vocabulary helps the model converge faster and this is where character based approach is much useful. Karpathy’s early [char-RNNs for generating Shakespeare](https://github.com/karpathy/char-rnn) relied on this simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc406f4",
   "metadata": {},
   "source": [
    "#### But Then What’s the Drawback?\n",
    "\n",
    "1. **Sequence Length Explosion:**\n",
    "\n",
    "Consider the below example:\n",
    "\n",
    "```bash\n",
    "hello world\n",
    "```\n",
    "\n",
    "If we see here clearly there are `11` character level tokens. Now imagine if we are doing some real life language modelling hence then we need real sentences, paragraphs or books and then model will need to process much longer sequences. Longer sequence will mean more memory, more training time, fewer tokens per batch (slower learning) and also more attention computation (quadratic cost). Hence **this is the single biggest reason modern LLMs don’t use pure character tokenization.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e6f3a",
   "metadata": {},
   "source": [
    "### 1.2 Approach-2: Word Based Tokenization\n",
    "\n",
    "After failure of character based tokenization failure the researchers thought that **A text is just a sequence of words. So what if we tokenize the text by splitting it into words.** This idea is very natural because that’s how humans think but then machines aren't humans and they don't think in words and that's why this mismatch turned word-based tokenization from NLP’s first “obvious solution” into one of its biggest limitations.<br>\n",
    "\n",
    "#### What is Word-Based Tokenization?\n",
    "\n",
    "In simple explanation word tokenization splits text at whitespace and punctuation into words. For example:<br>\n",
    "\n",
    "```bash\n",
    "Input:   \"Hello, world! I’m learning NLP.\"\n",
    "Tokens:  [\"Hello\", \"world\", \"I’m\", \"learning\", \"NLP\"]\n",
    "```\n",
    "\n",
    "The tokenizer removes punctuation or isolates it and then everything else becomes a word. After this each unique word becomes a vocabulary entry and then each word maps to an integer ID and finally model processes sequences of those integer IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ee9ff",
   "metadata": {},
   "source": [
    "#### How This Approach Fails:\n",
    "\n",
    "1. **Vocabulary Explosion:**\n",
    "\n",
    "When every unique word becomes an entry in the vocabulary then even simple variations produce new tokens. For example words like below:\n",
    "\n",
    "```css\n",
    "apple\n",
    "apples\n",
    "Apple\n",
    "APPLE\n",
    "apple's\n",
    "apple-like\n",
    "```\n",
    "will generate new tokens with even simple variations and this will result in size explosion.\n",
    "\n",
    "\n",
    "2. **Out of Vocabulary (OOV):**\n",
    "\n",
    "If a word never appeared during training the model cannot represent it. Hence the model can’t learn representations for words that don’t exist in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f341da",
   "metadata": {},
   "source": [
    "### 1.3 Approach-3: Byte Pair Encoding (BPE)\n",
    "\n",
    "If you reached till here then now you must be wondering\n",
    "- How do LLMs break text into tokens?\n",
    "- How do they avoid exploding vocabulary sizes?\n",
    "- How do they handle unknown words?\n",
    "\n",
    "The answer of all these questions is **Byte Pair Encoding** which is one of the most influential algorithms in modern NLP tokenization.\n",
    "\n",
    "#### Defining BPE\n",
    "\n",
    "Basically BPE learns the most frequent pairs of symbols and merges them into longer symbols iteratively. First we beging with individual bytes or individual characters (depending on implementation) then repeatedly merge the most common adjacent pairs until our vocabulary reaches the target size.<br>\n",
    "This results in common words become single tokens and rare words are decomposed into multiple subword chunks and due to this all texts become representable because we start from raw bytes.\n",
    "\n",
    "#### Building BPE With Example\n",
    "\n",
    "Consider the example:\n",
    "\n",
    "```css\n",
    "banana bandana ban\n",
    "```\n",
    "\n",
    "- **Step-1: Add word boundary markers**\n",
    "\n",
    "We append `_` at the end of each word (this prevents accidental merges across word boundaries). Now our new corpus becomes:\n",
    "\n",
    "```css\n",
    "b a n a n a _\n",
    "b a n d a n a _\n",
    "b a n _\n",
    "```\n",
    "\n",
    "- **Step-2: Count Initial Pair Frequencies**\n",
    "\n",
    "Now we break each word into pairs. The word `b a n a n a _` becomes pair like\n",
    "```css\n",
    "(b a), (a n), (n a), (a n), (n a), (a _)\n",
    "```\n",
    "\n",
    "Word `b a n d a n a _` becomes\n",
    "\n",
    "```css\n",
    "(b a), (a n), (n d), (d a), (a n), (n a), (a _)\n",
    "```\n",
    "\n",
    "Word `b a n _` becomes\n",
    "\n",
    "```css\n",
    "(b a), (a n), (n _)\n",
    "```\n",
    "\n",
    "The frequency of each word is as:\n",
    "\n",
    "```css\n",
    "| Pair    | Count |\n",
    "| ------- | ----- |\n",
    "| **b a** | 3     |\n",
    "| **a n** | 5     |\n",
    "| **n a** | 3     |\n",
    "| **a _** | 2     |\n",
    "| n d     | 1     |\n",
    "| d a     | 1     |\n",
    "| n _     | 1     |\n",
    "```\n",
    "\n",
    "As we can see above most frequent pair is `an`\n",
    "\n",
    "\n",
    "- **Step-3: Apply First Merge (\"an\")**\n",
    "\n",
    "Now we uppdate the corpus as:\n",
    "\n",
    "a) Word 1: `b a n a n a _` which becomes `b an a n a _` and after merging second `a n`: `b an an a _`\n",
    "\n",
    "b) Word 2: `b a n d a n a _` which becomes `b an d an a _`.\n",
    "\n",
    "c) Word 3: `b a n _` which becomes `b an _`\n",
    "\n",
    "So after first merge we have:\n",
    "\n",
    "```css\n",
    "b an an a _\n",
    "b an d an a _\n",
    "b an _\n",
    "```\n",
    "\n",
    "\n",
    "- **Step 4: Recount Pairs**\n",
    "\n",
    "Now we will extract pairs again:\n",
    "\n",
    "a) Word: `b an an a _` which pairs as\n",
    "\n",
    "```css\n",
    "(b an), (an an), (an a), (a _)\n",
    "```\n",
    "\n",
    "b) Word: `b an d an a _` now pairs as\n",
    "\n",
    "```css\n",
    "(b an), (an d), (d an), (an a), (a _)\n",
    "```\n",
    "\n",
    "c) Word: `b an _` pairs as\n",
    "\n",
    "```css\n",
    "(b an), (an _)\n",
    "```\n",
    "The table becomes now:\n",
    "\n",
    "```css\n",
    "\n",
    "| Pair     | Count |\n",
    "| -------- | ----- |\n",
    "| **b an** | 3     |\n",
    "| **an a** | 2     |\n",
    "| **a _**  | 2     |\n",
    "| an an    | 1     |\n",
    "| an d     | 1     |\n",
    "| d an     | 1     |\n",
    "| an _     | 1     |\n",
    "```\n",
    "\n",
    "The most frequent pair is now \"ban\" so the **second merge** will be **b + an = ban**.\n",
    "\n",
    "- **Step 5: Apply Second Merge (\"ban\")**\n",
    "\n",
    "Now reapply merge in all words:\n",
    "\n",
    "a) Word 1: `b an an a _` becomes `ban an a _`\n",
    "\n",
    "b) Word 2: `b an d an a _` becomes `ban d an a _`\n",
    "\n",
    "c) Word 3: `b an _` becomes `ban _`\n",
    "\n",
    "\n",
    "Now the updated corpus is\n",
    "\n",
    "```css\n",
    "ban an a _\n",
    "ban d an a _\n",
    "ban _\n",
    "```\n",
    "\n",
    "- **Step 6: Recount Pairs Again**\n",
    "\n",
    "a) We have word 1 pairs as `(ban an), (an a), (a _)`.\n",
    "\n",
    "b) Word 2 pairs as `(ban d), (d an), (an a), (a _)`\n",
    "\n",
    "c) Word 3 pairs as `(ban _)`.\n",
    "\n",
    "\n",
    "Now the count is as follows:\n",
    "\n",
    "\n",
    "```css\n",
    "| Pair     | Count |\n",
    "| -------- | ----- |\n",
    "| **an a** | 2     |\n",
    "| **a _**  | 2     |\n",
    "| ban an   | 1     |\n",
    "| ban d    | 1     |\n",
    "| d an     | 1     |\n",
    "| ban _    | 1     |\n",
    "```\n",
    "\n",
    "After this merge we have count of two pairs tie as `an a` and `a _`. We can pick *either* but in practice BPE picks deterministically.<br>\n",
    "First we pick `an a` to make `ana`. This is a very natural subword which is “ana” and common in “banana”.\n",
    "\n",
    "- **Step 6: Apply Merge (\"ana\")**\n",
    "\n",
    "After applying merge we update corpus as\n",
    "\n",
    "a) Word 1: `ban an a _` becomes `ban ana _`\n",
    "\n",
    "b) Word 2: `ban d an a _` becomes `ban d ana _`\n",
    "\n",
    "c) But word 3 remains unchanged: `ban _`\n",
    "\n",
    "Hence now new updated corpus us:\n",
    "\n",
    "```css\n",
    "ban ana _\n",
    "ban d ana _\n",
    "ban _\n",
    "```\n",
    "\n",
    "Which already looks very “natural” as original word `ban` + `ana` as *banana* and `ban` + `d` + `ana` → *bandana*\n",
    "\n",
    "\n",
    "- **Step 7: Recount Pairs**\n",
    "\n",
    "Now we have pairs as `(ban ana)`, `(ana _)`, `(ban d)`, `(d ana)`, `(ban _)` which has count of:\n",
    "\n",
    "```bash\n",
    "| Pair      | Count |\n",
    "| --------- | ----- |\n",
    "| **ana _** | 2     |\n",
    "| ban ana   | 1     |\n",
    "| ban d     | 1     |\n",
    "| d ana     | 1     |\n",
    "| ban _     | 1     |\n",
    "```\n",
    "\n",
    "with the most frequent word now `ana + _` as `ana_` which creates the full subword for ending “ana_”. But typically we stop merging once we get meaningful subunits e.g. 10k merges for a real tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e800e",
   "metadata": {},
   "source": [
    "## 2. BPE Tokenizer (Minimal Working Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f00bd",
   "metadata": {},
   "source": [
    "### 2.1 Training BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558cb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8567c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(tokens_list):\n",
    "    pairs = Counter()# Count frequency of adjacent token pairs\n",
    "    for tokens in tokens_list:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[(tokens[i], tokens[i+1])] += 1# Incrementing the count for the token pair\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68cb14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(tokens_list, merge_pair):\n",
    "    new_list = []# List to hold the updated token lists\n",
    "    bigram = \" \".join(merge_pair)# Create a regex pattern to match the bigram\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(bigram) + r'(?!\\S)')# Regex to match the bigram as a whole word\n",
    "\n",
    "    for tokens in tokens_list:\n",
    "        text = \" \".join(tokens)# Join tokens into a single string\n",
    "        merged = pattern.sub(\"\".join(merge_pair), text)# Replace bigram with merged token\n",
    "        new_list.append(merged.split())# Split back into tokens and add to new list\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a202ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(corpus, num_merges=10):\n",
    "    # Split corpus into space-separated words, adding a special end-of-word mark\n",
    "    tokens_list = [[c for c in word] for word in corpus.split()]\n",
    "    merges = []# List to hold the merge operations\n",
    "    for _ in range(num_merges):\n",
    "        stats = get_stats(tokens_list)# Get frequency of adjacent token pairs\n",
    "        if not stats:\n",
    "            break\n",
    "        # Most frequent pair\n",
    "        best = max(stats, key=stats.get)\n",
    "        merges.append(best)\n",
    "\n",
    "        # Merge it across corpus\n",
    "        tokens_list = merge_tokens(tokens_list, best)\n",
    "\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2b774",
   "metadata": {},
   "source": [
    "### 2.2 Applying BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73a97326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode(word, merges):\n",
    "    tokens = [c for c in word]# Initial tokens are characters\n",
    "    # Create a dictionary for quick lookup of merges\n",
    "    merge_dict = {\"\".join(pair): pair for pair in merges}\n",
    "    # Repeat merging until no more merges can be applied\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False# Flag to check if any merge happened\n",
    "        i = 0# Index to traverse tokens\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = tokens[i] + tokens[i+1]\n",
    "            if pair in merge_dict:# If this merged symbol was learned\n",
    "                tokens[i:i+2] = [pair]# Merge them\n",
    "                changed = True\n",
    "            else:\n",
    "                i += 1# Move to the next token\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec4aa3",
   "metadata": {},
   "source": [
    "### 2.3 Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b090f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_decode(tokens):\n",
    "    output = \"\"# Initialize output string\n",
    "    for t in tokens:\n",
    "        output += t# Concatenate tokens\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e88ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"banana bandana ban\"\n",
    "merges = train_bpe(corpus, num_merges=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8b61d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merges:\n",
      "('a', 'n')\n",
      "('b', 'an')\n",
      "('an', 'a')\n",
      "('ban', 'ana')\n",
      "('ban', 'd')\n",
      "('band', 'ana')\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned merges:\")\n",
    "for m in merges:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05792c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding 'banana':\n",
      "['b', 'ana', 'n', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEncoding 'banana':\")\n",
    "encoded = bpe_encode(\"banana\", merges)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b59288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoding:\n",
      "banana\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDecoding:\")\n",
    "print(bpe_decode(encoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
